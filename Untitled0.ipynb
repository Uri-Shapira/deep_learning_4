{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMd4+PS5HR2vxdLlYyDhEJq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uri-Shapira/deep_learning_4/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRQAQ1-LcNUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWvq_jeScZFu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "a8c62494-bf38-41da-84af-aad68abaf17d"
      },
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transforms.ToTensor())\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transforms.ToTensor())\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/170498071 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:02, 72398897.74it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6hI6kXRcrkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plane = trainset.class_to_idx['airplane']\n",
        "frog = trainset.class_to_idx['frog']\n",
        "\n",
        "def filter_plane_frog_classes(images):\n",
        "  result = []\n",
        "  for idx, x in enumerate(images):\n",
        "    arr, target = x\n",
        "    if (target == plane) or (target == frog):\n",
        "      result.append(idx)\n",
        "  return result\n",
        "\n",
        "frog_plane_trainset = torch.utils.data.Subset(trainset, filter_plane_frog_classes(trainset))\n",
        "frog_plane_testset = torch.utils.data.Subset(testset, filter_plane_frog_classes(testset))\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(frog_plane_trainset, batch_size=32,\n",
        "                                          shuffle=True)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(frog_plane_testset, batch_size=32,\n",
        "                                         shuffle=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ilwqi_KPcuS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "\n",
        "        # now a few fully connected layers\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32zCpBPccw42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################################################\n",
        "#########   DEFINE OPTIMIZERS ##########################\n",
        "########################################################\n",
        "class MyOptimizer:\n",
        "\n",
        "    min_step_list = []\n",
        "    max_step_list = []\n",
        "    v = None\n",
        "    gi = None\n",
        "    epsilon = 0.00000001\n",
        "\n",
        "    # name parameter to choose which optimizer\n",
        "    def __init__(self, parameters, learning_rate, name):\n",
        "        self.parameters = parameters\n",
        "        self.learning_rate = learning_rate\n",
        "        self.param_groups = []\n",
        "        self.name = name\n",
        "        # maybe not hardcode?\n",
        "        self.momentum = 0.5\n",
        "        self.beta = 0.01\n",
        "        param_groups = list(self.parameters)\n",
        "        if not isinstance(param_groups[0], dict):\n",
        "            param_groups = [{'params': param_groups}]\n",
        "        for param_group in param_groups:\n",
        "            self.add_param_group(param_group)\n",
        "\n",
        "    def add_param_group(self, param_group):\n",
        "        params = param_group['params']\n",
        "        if isinstance(params, torch.Tensor):\n",
        "            param_group['params'] = [params]\n",
        "        else:\n",
        "            param_group['params'] = list(params)\n",
        "\n",
        "        param_set = set()\n",
        "        for group in self.param_groups:\n",
        "            param_set.update(set(group['params']))\n",
        "\n",
        "        self.param_groups.append(param_group)\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    p.grad.detach_()\n",
        "                    p.grad.zero_()\n",
        "\n",
        "    def sgd(self):\n",
        "        for group in self.param_groups:\n",
        "            params = group['params']\n",
        "            for p in params:\n",
        "                if p.grad is None:\n",
        "                    print('none')\n",
        "                    continue\n",
        "                dp = p.grad.data\n",
        "                p.data = p.data.add(-self.learning_rate, dp)\n",
        "\n",
        "    def SGD_with_momentum(self):\n",
        "        v = 0\n",
        "        for group in self.param_groups:\n",
        "            params = group['params']\n",
        "            for p in params:\n",
        "                if p.grad is None:\n",
        "                    print('none')\n",
        "                    continue\n",
        "                dp = p.grad.data\n",
        "                if self.momentum != 0:\n",
        "                    buf = torch.clone(dp).detach()\n",
        "                    buf.mul_(self.momentum).add_(1 - self.beta, dp)\n",
        "                p.data = p.data.add(-self.learning_rate, dp)\n",
        "\n",
        "    # def RMSProp(self):\n",
        "\n",
        "    def adam(self):\n",
        "        for group in self.param_groups:\n",
        "            params = group['params']\n",
        "            if self.v is None:\n",
        "                self.v = [None for i in params]\n",
        "            if self.gi is None:\n",
        "                self.gi = [None for i in params]\n",
        "            for i, p in enumerate(params):\n",
        "                if p.grad is None:\n",
        "                    print('none')\n",
        "                    continue\n",
        "                dp = p.grad.data\n",
        "                if self.gi[i] is None:\n",
        "                    self.gi[i] = torch.zeros_like(dp)\n",
        "                if self.v[i] is None:\n",
        "                    self.v[i] = dp.clone().detach()\n",
        "                else:\n",
        "                    self.v[i] = self.v[i] * self.momentum + (1 - self.momentum) * dp\n",
        "                b = 0.999\n",
        "                self.gi[i] = b * self.gi[i] + (1 - b) * (dp ** 2)\n",
        "                temp = self.v[i] / torch.sqrt(self.gi[i] + self.epsilon)\n",
        "                self.add_min_max_steps(temp)\n",
        "                p.data = p.data.add(-self.learning_rate, self.v[i] / torch.sqrt(self.gi[i] + self.epsilon))\n",
        "\n",
        "    def add_min_max_steps(self, steps):\n",
        "        flatten_steps = torch.flatten(steps)\n",
        "        self.min_step_list.append(torch.topk(input=flatten_steps, dim=0, k=2, largest=False, sorted=True)[0].tolist())\n",
        "        self.max_step_list.append(torch.topk(input=flatten_steps, dim=0, k=2, largest=True, sorted=True)[0].tolist())\n",
        "        return\n",
        "\n",
        "    def step(self):\n",
        "        if self.name == 'sgd':\n",
        "            self.sgd()\n",
        "        elif self.name == 'SGD_with_momentum':\n",
        "            self.SGD_with_momentum()\n",
        "        elif self.name == 'adam':\n",
        "          self.adam()\n",
        "        elif self.name == 'RMSProp':\n",
        "          self.RMSProp()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvxRZDo-c26O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################################################\n",
        "#########   TRAIN             ##########################\n",
        "########################################################\n",
        "\n",
        "# use cuda?\n",
        "def get_test_loss(net):\n",
        "    test_loss = 0.0\n",
        "    for i, data in enumerate(testloader, 0):\n",
        "        inputs, labels = data\n",
        "        outputs = net(inputs)\n",
        "        current_loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        test_loss += current_loss.item()\n",
        "    return test_loss / (i + 1)\n",
        "\n",
        "def train(net, trainloader, step_name, num_of_epochs=100):\n",
        "  # define loss function\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  # define the optimizer\n",
        "  optimizer = MyOptimizer(net.parameters(), 0.001, step_name)\n",
        "  ###optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "  loss_train = []\n",
        "  loss_test = []\n",
        "  for epoch in range(num_of_epochs):\n",
        "      epoch_loss_sum = 0.0\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "          # get the inputs\n",
        "          inputs, labels = data\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = net(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "\n",
        "        # will run optimizer based on step_name parameter\n",
        "          optimizer.step()\n",
        "          # optimizer.SGD_with_momentum(0.5, 0.01)\n",
        "\n",
        "          # print statistics\n",
        "          curr_loss = loss.item()\n",
        "          running_loss += curr_loss\n",
        "          epoch_loss_sum += curr_loss\n",
        "          if i % 200 == 0:\n",
        "              print('[%d, %5d] Train Loss: %.3f' %\n",
        "                    (epoch + 1, i + 1, running_loss / 200.0))\n",
        "              running_loss = 0.0\n",
        "      current_test_loss = get_test_loss(net)\n",
        "      print('[%d] Test Loss: %.3f' %(epoch + 1, current_test_loss))\n",
        "      loss_train.append(epoch_loss_sum / len(trainloader))\n",
        "      loss_test.append(current_test_loss)\n",
        "\n",
        "  print('Finished Training')\n",
        "  return loss_train, loss_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVXha5sCdF8y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "32032d72-349b-435c-ec8b-ad456df726ba"
      },
      "source": [
        "##### CREATE NETWORKS AND TRAIN WITH 4 OPTIMIZERS\n",
        "\n",
        "net = CNN()\n",
        "print(net)\n",
        "adam_loss_train, adam_loss_test = train(net, trainloader, 'adam')\n",
        "sgd_net = CNN()\n",
        "print(sgd_net)\n",
        "sgd_loss_train, sgd_loss_test = train(sgd_net, trainloader, 'sgd')\n",
        "sgd_momentum_net = CNN()\n",
        "print(sgd_momentum_net)\n",
        "sgd_momentum_loss_train, sgd_momentum_loss_test = train(sgd_momentum_net, trainloader, 'SGD_with_momentum')\n",
        "# RMS_net = CNN()\n",
        "# RMSProp_loss_train, RMSProp_loss_test = train(RMS_net, trainloader, 'RMSProp')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "[1,     1] Train Loss: 0.012\n",
            "[1,   201] Train Loss: 0.712\n",
            "[0] Test Loss: 0.320\n",
            "[2,     1] Train Loss: 0.002\n",
            "[2,   201] Train Loss: 0.334\n",
            "[1] Test Loss: 0.262\n",
            "[3,     1] Train Loss: 0.002\n",
            "[3,   201] Train Loss: 0.299\n",
            "[2] Test Loss: 0.249\n",
            "[4,     1] Train Loss: 0.001\n",
            "[4,   201] Train Loss: 0.269\n",
            "[3] Test Loss: 0.411\n",
            "[5,     1] Train Loss: 0.002\n",
            "[5,   201] Train Loss: 0.236\n",
            "[4] Test Loss: 0.211\n",
            "[6,     1] Train Loss: 0.001\n",
            "[6,   201] Train Loss: 0.236\n",
            "[5] Test Loss: 0.192\n",
            "[7,     1] Train Loss: 0.001\n",
            "[7,   201] Train Loss: 0.204\n",
            "[6] Test Loss: 0.239\n",
            "[8,     1] Train Loss: 0.001\n",
            "[8,   201] Train Loss: 0.195\n",
            "[7] Test Loss: 0.177\n",
            "[9,     1] Train Loss: 0.001\n",
            "[9,   201] Train Loss: 0.180\n",
            "[8] Test Loss: 0.257\n",
            "[10,     1] Train Loss: 0.001\n",
            "[10,   201] Train Loss: 0.174\n",
            "[9] Test Loss: 0.183\n",
            "[11,     1] Train Loss: 0.001\n",
            "[11,   201] Train Loss: 0.175\n",
            "[10] Test Loss: 0.173\n",
            "[12,     1] Train Loss: 0.000\n",
            "[12,   201] Train Loss: 0.153\n",
            "[11] Test Loss: 0.184\n",
            "[13,     1] Train Loss: 0.001\n",
            "[13,   201] Train Loss: 0.151\n",
            "[12] Test Loss: 0.165\n",
            "[14,     1] Train Loss: 0.000\n",
            "[14,   201] Train Loss: 0.152\n",
            "[13] Test Loss: 0.162\n",
            "[15,     1] Train Loss: 0.001\n",
            "[15,   201] Train Loss: 0.142\n",
            "[14] Test Loss: 0.188\n",
            "[16,     1] Train Loss: 0.000\n",
            "[16,   201] Train Loss: 0.144\n",
            "[15] Test Loss: 0.176\n",
            "[17,     1] Train Loss: 0.002\n",
            "[17,   201] Train Loss: 0.132\n",
            "[16] Test Loss: 0.164\n",
            "[18,     1] Train Loss: 0.000\n",
            "[18,   201] Train Loss: 0.135\n",
            "[17] Test Loss: 0.170\n",
            "[19,     1] Train Loss: 0.000\n",
            "[19,   201] Train Loss: 0.130\n",
            "[18] Test Loss: 0.176\n",
            "[20,     1] Train Loss: 0.000\n",
            "[20,   201] Train Loss: 0.123\n",
            "[19] Test Loss: 0.229\n",
            "[21,     1] Train Loss: 0.001\n",
            "[21,   201] Train Loss: 0.120\n",
            "[20] Test Loss: 0.186\n",
            "[22,     1] Train Loss: 0.002\n",
            "[22,   201] Train Loss: 0.109\n",
            "[21] Test Loss: 0.176\n",
            "[23,     1] Train Loss: 0.001\n",
            "[23,   201] Train Loss: 0.109\n",
            "[22] Test Loss: 0.201\n",
            "[24,     1] Train Loss: 0.000\n",
            "[24,   201] Train Loss: 0.100\n",
            "[23] Test Loss: 0.195\n",
            "[25,     1] Train Loss: 0.001\n",
            "[25,   201] Train Loss: 0.098\n",
            "[24] Test Loss: 0.238\n",
            "[26,     1] Train Loss: 0.000\n",
            "[26,   201] Train Loss: 0.090\n",
            "[25] Test Loss: 0.209\n",
            "[27,     1] Train Loss: 0.000\n",
            "[27,   201] Train Loss: 0.090\n",
            "[26] Test Loss: 0.222\n",
            "[28,     1] Train Loss: 0.001\n",
            "[28,   201] Train Loss: 0.082\n",
            "[27] Test Loss: 0.265\n",
            "[29,     1] Train Loss: 0.002\n",
            "[29,   201] Train Loss: 0.070\n",
            "[28] Test Loss: 0.204\n",
            "[30,     1] Train Loss: 0.000\n",
            "[30,   201] Train Loss: 0.069\n",
            "[29] Test Loss: 0.222\n",
            "[31,     1] Train Loss: 0.001\n",
            "[31,   201] Train Loss: 0.062\n",
            "[30] Test Loss: 0.221\n",
            "[32,     1] Train Loss: 0.000\n",
            "[32,   201] Train Loss: 0.057\n",
            "[31] Test Loss: 0.227\n",
            "[33,     1] Train Loss: 0.000\n",
            "[33,   201] Train Loss: 0.050\n",
            "[32] Test Loss: 0.267\n",
            "[34,     1] Train Loss: 0.000\n",
            "[34,   201] Train Loss: 0.048\n",
            "[33] Test Loss: 0.264\n",
            "[35,     1] Train Loss: 0.000\n",
            "[35,   201] Train Loss: 0.045\n",
            "[34] Test Loss: 0.295\n",
            "[36,     1] Train Loss: 0.000\n",
            "[36,   201] Train Loss: 0.039\n",
            "[35] Test Loss: 0.312\n",
            "[37,     1] Train Loss: 0.000\n",
            "[37,   201] Train Loss: 0.037\n",
            "[36] Test Loss: 0.270\n",
            "[38,     1] Train Loss: 0.000\n",
            "[38,   201] Train Loss: 0.031\n",
            "[37] Test Loss: 0.283\n",
            "[39,     1] Train Loss: 0.000\n",
            "[39,   201] Train Loss: 0.027\n",
            "[38] Test Loss: 0.274\n",
            "[40,     1] Train Loss: 0.000\n",
            "[40,   201] Train Loss: 0.021\n",
            "[39] Test Loss: 0.337\n",
            "[41,     1] Train Loss: 0.000\n",
            "[41,   201] Train Loss: 0.030\n",
            "[40] Test Loss: 0.409\n",
            "[42,     1] Train Loss: 0.000\n",
            "[42,   201] Train Loss: 0.024\n",
            "[41] Test Loss: 0.351\n",
            "[43,     1] Train Loss: 0.000\n",
            "[43,   201] Train Loss: 0.020\n",
            "[42] Test Loss: 0.354\n",
            "[44,     1] Train Loss: 0.000\n",
            "[44,   201] Train Loss: 0.016\n",
            "[43] Test Loss: 0.408\n",
            "[45,     1] Train Loss: 0.000\n",
            "[45,   201] Train Loss: 0.029\n",
            "[44] Test Loss: 0.398\n",
            "[46,     1] Train Loss: 0.000\n",
            "[46,   201] Train Loss: 0.014\n",
            "[45] Test Loss: 0.384\n",
            "[47,     1] Train Loss: 0.000\n",
            "[47,   201] Train Loss: 0.019\n",
            "[46] Test Loss: 0.352\n",
            "[48,     1] Train Loss: 0.000\n",
            "[48,   201] Train Loss: 0.015\n",
            "[47] Test Loss: 0.405\n",
            "[49,     1] Train Loss: 0.000\n",
            "[49,   201] Train Loss: 0.017\n",
            "[48] Test Loss: 0.438\n",
            "[50,     1] Train Loss: 0.000\n",
            "[50,   201] Train Loss: 0.005\n",
            "[49] Test Loss: 0.439\n",
            "[51,     1] Train Loss: 0.000\n",
            "[51,   201] Train Loss: 0.033\n",
            "[50] Test Loss: 0.427\n",
            "[52,     1] Train Loss: 0.000\n",
            "[52,   201] Train Loss: 0.028\n",
            "[51] Test Loss: 0.399\n",
            "[53,     1] Train Loss: 0.000\n",
            "[53,   201] Train Loss: 0.017\n",
            "[52] Test Loss: 0.401\n",
            "[54,     1] Train Loss: 0.000\n",
            "[54,   201] Train Loss: 0.022\n",
            "[53] Test Loss: 0.378\n",
            "[55,     1] Train Loss: 0.000\n",
            "[55,   201] Train Loss: 0.009\n",
            "[54] Test Loss: 0.493\n",
            "[56,     1] Train Loss: 0.000\n",
            "[56,   201] Train Loss: 0.009\n",
            "[55] Test Loss: 0.424\n",
            "[57,     1] Train Loss: 0.000\n",
            "[57,   201] Train Loss: 0.019\n",
            "[56] Test Loss: 0.398\n",
            "[58,     1] Train Loss: 0.000\n",
            "[58,   201] Train Loss: 0.002\n",
            "[57] Test Loss: 0.466\n",
            "[59,     1] Train Loss: 0.000\n",
            "[59,   201] Train Loss: 0.002\n",
            "[58] Test Loss: 0.484\n",
            "[60,     1] Train Loss: 0.000\n",
            "[60,   201] Train Loss: 0.001\n",
            "[59] Test Loss: 0.542\n",
            "[61,     1] Train Loss: 0.000\n",
            "[61,   201] Train Loss: 0.000\n",
            "[60] Test Loss: 0.544\n",
            "[62,     1] Train Loss: 0.000\n",
            "[62,   201] Train Loss: 0.051\n",
            "[61] Test Loss: 0.324\n",
            "[63,     1] Train Loss: 0.000\n",
            "[63,   201] Train Loss: 0.023\n",
            "[62] Test Loss: 0.362\n",
            "[64,     1] Train Loss: 0.000\n",
            "[64,   201] Train Loss: 0.004\n",
            "[63] Test Loss: 0.410\n",
            "[65,     1] Train Loss: 0.000\n",
            "[65,   201] Train Loss: 0.007\n",
            "[64] Test Loss: 0.371\n",
            "[66,     1] Train Loss: 0.000\n",
            "[66,   201] Train Loss: 0.020\n",
            "[65] Test Loss: 0.383\n",
            "[67,     1] Train Loss: 0.000\n",
            "[67,   201] Train Loss: 0.003\n",
            "[66] Test Loss: 0.429\n",
            "[68,     1] Train Loss: 0.000\n",
            "[68,   201] Train Loss: 0.001\n",
            "[67] Test Loss: 0.456\n",
            "[69,     1] Train Loss: 0.000\n",
            "[69,   201] Train Loss: 0.000\n",
            "[68] Test Loss: 0.491\n",
            "[70,     1] Train Loss: 0.000\n",
            "[70,   201] Train Loss: 0.000\n",
            "[69] Test Loss: 0.513\n",
            "[71,     1] Train Loss: 0.000\n",
            "[71,   201] Train Loss: 0.000\n",
            "[70] Test Loss: 0.526\n",
            "[72,     1] Train Loss: 0.000\n",
            "[72,   201] Train Loss: 0.000\n",
            "[71] Test Loss: 0.542\n",
            "[73,     1] Train Loss: 0.000\n",
            "[73,   201] Train Loss: 0.000\n",
            "[72] Test Loss: 0.561\n",
            "[74,     1] Train Loss: 0.000\n",
            "[74,   201] Train Loss: 0.000\n",
            "[73] Test Loss: 0.576\n",
            "[75,     1] Train Loss: 0.000\n",
            "[75,   201] Train Loss: 0.000\n",
            "[74] Test Loss: 0.594\n",
            "[76,     1] Train Loss: 0.000\n",
            "[76,   201] Train Loss: 0.000\n",
            "[75] Test Loss: 0.604\n",
            "[77,     1] Train Loss: 0.000\n",
            "[77,   201] Train Loss: 0.000\n",
            "[76] Test Loss: 0.663\n",
            "[78,     1] Train Loss: 0.000\n",
            "[78,   201] Train Loss: 0.174\n",
            "[77] Test Loss: 0.336\n",
            "[79,     1] Train Loss: 0.000\n",
            "[79,   201] Train Loss: 0.026\n",
            "[78] Test Loss: 0.343\n",
            "[80,     1] Train Loss: 0.000\n",
            "[80,   201] Train Loss: 0.018\n",
            "[79] Test Loss: 0.373\n",
            "[81,     1] Train Loss: 0.000\n",
            "[81,   201] Train Loss: 0.005\n",
            "[80] Test Loss: 0.450\n",
            "[82,     1] Train Loss: 0.000\n",
            "[82,   201] Train Loss: 0.010\n",
            "[81] Test Loss: 0.485\n",
            "[83,     1] Train Loss: 0.000\n",
            "[83,   201] Train Loss: 0.001\n",
            "[82] Test Loss: 0.517\n",
            "[84,     1] Train Loss: 0.000\n",
            "[84,   201] Train Loss: 0.000\n",
            "[83] Test Loss: 0.543\n",
            "[85,     1] Train Loss: 0.000\n",
            "[85,   201] Train Loss: 0.000\n",
            "[84] Test Loss: 0.576\n",
            "[86,     1] Train Loss: 0.000\n",
            "[86,   201] Train Loss: 0.000\n",
            "[85] Test Loss: 0.582\n",
            "[87,     1] Train Loss: 0.000\n",
            "[87,   201] Train Loss: 0.000\n",
            "[86] Test Loss: 0.607\n",
            "[88,     1] Train Loss: 0.000\n",
            "[88,   201] Train Loss: 0.000\n",
            "[87] Test Loss: 0.620\n",
            "[89,     1] Train Loss: 0.000\n",
            "[89,   201] Train Loss: 0.000\n",
            "[88] Test Loss: 0.634\n",
            "[90,     1] Train Loss: 0.000\n",
            "[90,   201] Train Loss: 0.000\n",
            "[89] Test Loss: 0.650\n",
            "[91,     1] Train Loss: 0.000\n",
            "[91,   201] Train Loss: 0.000\n",
            "[90] Test Loss: 0.668\n",
            "[92,     1] Train Loss: 0.000\n",
            "[92,   201] Train Loss: 0.000\n",
            "[91] Test Loss: 0.678\n",
            "[93,     1] Train Loss: 0.000\n",
            "[93,   201] Train Loss: 0.000\n",
            "[92] Test Loss: 0.694\n",
            "[94,     1] Train Loss: 0.000\n",
            "[94,   201] Train Loss: 0.000\n",
            "[93] Test Loss: 0.705\n",
            "[95,     1] Train Loss: 0.000\n",
            "[95,   201] Train Loss: 0.000\n",
            "[94] Test Loss: 0.724\n",
            "[96,     1] Train Loss: 0.000\n",
            "[96,   201] Train Loss: 0.000\n",
            "[95] Test Loss: 0.728\n",
            "[97,     1] Train Loss: 0.000\n",
            "[97,   201] Train Loss: 0.000\n",
            "[96] Test Loss: 0.741\n",
            "[98,     1] Train Loss: 0.000\n",
            "[98,   201] Train Loss: 0.000\n",
            "[97] Test Loss: 0.753\n",
            "[99,     1] Train Loss: 0.000\n",
            "[99,   201] Train Loss: 0.000\n",
            "[98] Test Loss: 0.770\n",
            "[100,     1] Train Loss: 0.000\n",
            "[100,   201] Train Loss: 0.000\n",
            "[99] Test Loss: 0.779\n",
            "Finished Training\n",
            "CNN(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "[1,     1] Train Loss: 0.011\n",
            "[1,   201] Train Loss: 2.245\n",
            "[0] Test Loss: 2.130\n",
            "[2,     1] Train Loss: 0.011\n",
            "[2,   201] Train Loss: 2.070\n",
            "[1] Test Loss: 1.858\n",
            "[3,     1] Train Loss: 0.009\n",
            "[3,   201] Train Loss: 1.401\n",
            "[2] Test Loss: 0.721\n",
            "[4,     1] Train Loss: 0.004\n",
            "[4,   201] Train Loss: 0.703\n",
            "[3] Test Loss: 0.681\n",
            "[5,     1] Train Loss: 0.003\n",
            "[5,   201] Train Loss: 0.677\n",
            "[4] Test Loss: 0.675\n",
            "[6,     1] Train Loss: 0.003\n",
            "[6,   201] Train Loss: 0.667\n",
            "[5] Test Loss: 0.658\n",
            "[7,     1] Train Loss: 0.003\n",
            "[7,   201] Train Loss: 0.656\n",
            "[6] Test Loss: 0.648\n",
            "[8,     1] Train Loss: 0.003\n",
            "[8,   201] Train Loss: 0.648\n",
            "[7] Test Loss: 0.639\n",
            "[9,     1] Train Loss: 0.003\n",
            "[9,   201] Train Loss: 0.636\n",
            "[8] Test Loss: 0.633\n",
            "[10,     1] Train Loss: 0.003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVlCOtV6laVM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "outputId": "6f52be22-c7f5-489a-eb9c-bb250e401cdc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(sgd_loss_train, label=\"sgd loss train\")\n",
        "plt.plot(sgd_loss_test, label=\"sgd loss test\")\n",
        "plt.plot(sgd_momentum_loss_train, label=\"sgd momentum loss train\")\n",
        "plt.plot(sgd_momentum_loss_test, label=\"sgd momentum loss test\")\n",
        "plt.plot(adam_loss_train, label=\"adam loss train\")\n",
        "plt.plot(adam_loss_test, label=\"adam loss test\")\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7c6831aa150e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msgd_momentum_loss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sgd momentum loss train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msgd_momentum_loss_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sgd momentum loss test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madam_loss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam loss train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madam_loss_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam loss test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'adam_loss_train' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}