{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMeqlnmyIj7ughr+eKYe0/L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uri-Shapira/deep_learning_4/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRQAQ1-LcNUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWvq_jeScZFu",
        "colab_type": "code",
        "outputId": "9e4bfa54-5eba-47c8-9fc5-34d1943a6cdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transforms.ToTensor())\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transforms.ToTensor())\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 98%|█████████▊| 167747584/170498071 [00:12<00:00, 16806571.54it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6hI6kXRcrkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plane = trainset.class_to_idx['airplane']\n",
        "frog = trainset.class_to_idx['frog']\n",
        "\n",
        "def filter_plane_frog_classes(images):\n",
        "  result = []\n",
        "  for idx, x in enumerate(images):\n",
        "    arr, target = x\n",
        "    if (target == plane) or (target == frog):\n",
        "      result.append(idx)\n",
        "  return result\n",
        "\n",
        "frog_plane_trainset = torch.utils.data.Subset(trainset, filter_plane_frog_classes(trainset))\n",
        "frog_plane_testset = torch.utils.data.Subset(testset, filter_plane_frog_classes(testset))\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(frog_plane_trainset, batch_size=32,\n",
        "                                          shuffle=True)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(frog_plane_testset, batch_size=32,\n",
        "                                         shuffle=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ilwqi_KPcuS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "\n",
        "        # now a few fully connected layers\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32zCpBPccw42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################################################\n",
        "#########   DEFINE OPTIMIZERS ##########################\n",
        "########################################################\n",
        "class MyOptimizer:\n",
        "\n",
        "    def __init__(self, parameters):\n",
        "        self.parameters = parameters\n",
        "        self.learning_rate = 0.001\n",
        "        self.param_groups = []\n",
        "        param_groups = list(self.parameters)\n",
        "        if not isinstance(param_groups[0], dict):\n",
        "            param_groups = [{'params': param_groups}]\n",
        "        for param_group in param_groups:\n",
        "            self.add_param_group(param_group)\n",
        "\n",
        "    def add_param_group(self, param_group):\n",
        "        params = param_group['params']\n",
        "        if isinstance(params, torch.Tensor):\n",
        "            param_group['params'] = [params]\n",
        "        else:\n",
        "            param_group['params'] = list(params)\n",
        "        param_set = set()\n",
        "        for group in self.param_groups:\n",
        "            param_set.update(set(group['params']))\n",
        "        self.param_groups.append(param_group)\n",
        "  \n",
        "    def zero_grad(self):\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    p.grad.detach_()\n",
        "                    p.grad.zero_()\n",
        "    def step(self):\n",
        "      raise NotImplementedError\n",
        "\n",
        "class sgd(MyOptimizer):\n",
        "\n",
        "    def __init__(self, parameters):\n",
        "        MyOptimizer.__init__(self,parameters)\n",
        "\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            params = group['params']\n",
        "            for p in params:\n",
        "                if p.grad is None:\n",
        "                    print('none')\n",
        "                    continue\n",
        "                dp = p.grad.data\n",
        "                p.data = p.data.add(-self.learning_rate, dp)\n",
        "\n",
        "class SGD_with_momentum(MyOptimizer):\n",
        "    \n",
        "    def __init__(self, parameters):\n",
        "        self.momentum = 0.5\n",
        "        self.beta = 0.01 \n",
        "        MyOptimizer.__init__(self,parameters)\n",
        "    \n",
        "    def step(self):\n",
        "        v = 0\n",
        "        for group in self.param_groups:\n",
        "            params = group['params']\n",
        "            for p in params:\n",
        "                if p.grad is None:\n",
        "                    print('none')\n",
        "                    continue\n",
        "                dp = p.grad.data\n",
        "                if self.momentum != 0:\n",
        "                    buf = torch.clone(dp).detach()\n",
        "                    buf.mul_(self.momentum).add_(1 - self.beta, dp)\n",
        "                p.data = p.data.add(-self.learning_rate, dp)\n",
        "\n",
        "class ADAM(MyOptimizer):\n",
        "\n",
        "  def __init__(self, parameters):\n",
        "      MyOptimizer.__init__(self,parameters)\n",
        "      self.v = None\n",
        "      self.gi = None\n",
        "      self.momentum = 0.5\n",
        "      self.epsilon = 0.00000001\n",
        "      self.min_lr_list = []\n",
        "      self.max_lr_list = []\n",
        "\n",
        "  def step(self):\n",
        "        minlr = np.Infinity\n",
        "        maxlr = 0\n",
        "        for group in self.param_groups:\n",
        "            params = group['params']\n",
        "            if self.v is None:\n",
        "                self.v = [None for i in params]\n",
        "            if self.gi is None:\n",
        "                self.gi = [None for i in params]\n",
        "            for i, p in enumerate(params):\n",
        "                if p.grad is None:\n",
        "                    print('none')\n",
        "                    continue\n",
        "                dp = p.grad.data\n",
        "                if self.gi[i] is None:\n",
        "                    self.gi[i] = torch.zeros_like(dp)\n",
        "                if self.v[i] is None:\n",
        "                    self.v[i] = dp.clone().detach()\n",
        "                else:\n",
        "                    self.v[i] = self.v[i] * self.momentum + (1 - self.momentum) * dp\n",
        "                b = 0.999\n",
        "                self.gi[i] = b * self.gi[i] + (1 - b) * (dp ** 2)\n",
        "                # temp = self.v[i] / torch.sqrt(self.gi[i] + self.epsilon)\n",
        "                # print(\"=====================================\")\n",
        "                # print(temp)\n",
        "                # if torch.clamp(temp,max=value) > maxlr:\n",
        "                #   maxlr = torch.clamp(temp,max=value)\n",
        "                # if torch.clamp(temp,min=value) < minlr:\n",
        "                #   minlr = torch.clamp(temp,min=value)\n",
        "                p.data = p.data.add(-self.learning_rate, self.v[i] / torch.sqrt(self.gi[i] + self.epsilon))\n",
        "        # self.min_lr_list.append(minlr) \n",
        "        # self.max_lr_list.append(maxlr)\n",
        "\n",
        "class RMSProp(MyOptimizer):\n",
        "\n",
        "    def __init__(self, parameters):\n",
        "        MyOptimizer.__init__(self,parameters)\n",
        "        self.beta = 0.01\n",
        "        self.epsilon = 0.00000001\n",
        "        self.gi = None\n",
        "\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "          params = group['params']\n",
        "          if self.beta is None:\n",
        "            self.beta = [None for i in params]\n",
        "          if self.gi is None:\n",
        "            self.gi = [None for i in params]\n",
        "          for i, p in enumerate(params):\n",
        "            if p.grad is None:\n",
        "              print('none')\n",
        "              continue\n",
        "            dp = p.grad.data\n",
        "            if self.gi[i] is None:\n",
        "              self.gi[i] = torch.zeros_like(dp)\n",
        "            self.gi[i] = self.beta * self.gi[i] + (1 - self.beta) * (dp ** 2)\n",
        "            p.data = p.data.add(-self.learning_rate*dp/np.sqrt(self.gi[i]+self.epsilon))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvxRZDo-c26O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################################################\n",
        "#########   TRAIN             ##########################\n",
        "########################################################\n",
        "def get_test_loss(net):\n",
        "    test_loss = 0.0\n",
        "    for i, data in enumerate(testloader, 0):\n",
        "        inputs, labels = data\n",
        "        outputs = net(inputs)\n",
        "        current_loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        test_loss += current_loss.item()\n",
        "    return test_loss / (i + 1)\n",
        "\n",
        "def get_optimizer_type(step_name, parameters):\n",
        "    optimizer = None\n",
        "    if step_name == 'sgd':\n",
        "        optimizer = sgd(parameters)\n",
        "    elif step_name == 'sgd_with_momentum':\n",
        "        optimizer = SGD_with_momentum(parameters)\n",
        "    elif step_name == 'adam':\n",
        "        optimizer = ADAM(parameters)\n",
        "    else:\n",
        "        optimizer = RMSProp(parameters)\n",
        "    return optimizer\n",
        "\n",
        "def train(net, trainloader, step_name, num_of_epochs=100):\n",
        "    # define loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # define the optimizer\n",
        "    optimizer = get_optimizer_type(step_name, net.parameters())\n",
        "    loss_train = []\n",
        "    loss_test = []\n",
        "    for epoch in range(num_of_epochs):\n",
        "        epoch_loss_sum = 0.0\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs\n",
        "            inputs, labels = data\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "          # will run optimizer based on step_name parameter\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            curr_loss = loss.item()\n",
        "            running_loss += curr_loss\n",
        "            epoch_loss_sum += curr_loss\n",
        "            if i % 200 == 0:\n",
        "                print('[%d, %5d] Train Loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / 200.0))\n",
        "                running_loss = 0.0\n",
        "        current_test_loss = get_test_loss(net)\n",
        "        print('[%d] Test Loss: %.3f' %(epoch + 1, current_test_loss))\n",
        "        loss_train.append(epoch_loss_sum / len(trainloader))\n",
        "        loss_test.append(current_test_loss)\n",
        "\n",
        "    print('Finished Training')\n",
        "    return loss_train, loss_test, optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVXha5sCdF8y",
        "colab_type": "code",
        "outputId": "47413ad7-2c52-4ebd-8ca5-c361c59195ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "##### CREATE NETWORKS AND TRAIN WITH 4 OPTIMIZERS\n",
        "\n",
        "net = CNN()\n",
        "adam_loss_train, adam_loss_test, adam_optimizer = train(net, trainloader, 'adam')\n",
        "\n",
        "sgd_net = CNN()\n",
        "sgd_loss_train, sgd_loss_test, sgd_optimizer = train(sgd_net, trainloader, 'sgd')\n",
        "\n",
        "sgd_momentum_net = CNN()\n",
        "sgd_momentum_loss_train, sgd_momentum_loss_test, sgd_momentum_optimzer = train(sgd_momentum_net, trainloader, 'SGD_with_momentum')\n",
        "\n",
        "RMS_net = CNN()\n",
        "RMSProp_loss_train, RMSProp_loss_test, RMProp_optimizer = train(RMS_net, trainloader, 'RMSProp')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,     1] Train Loss: 0.011\n",
            "[1,   201] Train Loss: 0.680\n",
            "[1] Test Loss: 0.406\n",
            "[2,     1] Train Loss: 0.003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r170500096it [00:29, 16806571.54it/s]                               "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2,   201] Train Loss: 0.272\n",
            "[2] Test Loss: 0.255\n",
            "[3,     1] Train Loss: 0.001\n",
            "[3,   201] Train Loss: 0.232\n",
            "[3] Test Loss: 0.213\n",
            "[4,     1] Train Loss: 0.000\n",
            "[4,   201] Train Loss: 0.205\n",
            "[4] Test Loss: 0.279\n",
            "[5,     1] Train Loss: 0.002\n",
            "[5,   201] Train Loss: 0.186\n",
            "[5] Test Loss: 0.167\n",
            "[6,     1] Train Loss: 0.001\n",
            "[6,   201] Train Loss: 0.176\n",
            "[6] Test Loss: 0.241\n",
            "[7,     1] Train Loss: 0.002\n",
            "[7,   201] Train Loss: 0.159\n",
            "[7] Test Loss: 0.221\n",
            "[8,     1] Train Loss: 0.001\n",
            "[8,   201] Train Loss: 0.153\n",
            "[8] Test Loss: 0.206\n",
            "[9,     1] Train Loss: 0.000\n",
            "[9,   201] Train Loss: 0.144\n",
            "[9] Test Loss: 0.142\n",
            "[10,     1] Train Loss: 0.001\n",
            "[10,   201] Train Loss: 0.122\n",
            "[10] Test Loss: 0.164\n",
            "[11,     1] Train Loss: 0.001\n",
            "[11,   201] Train Loss: 0.117\n",
            "[11] Test Loss: 0.197\n",
            "[12,     1] Train Loss: 0.001\n",
            "[12,   201] Train Loss: 0.109\n",
            "[12] Test Loss: 0.141\n",
            "[13,     1] Train Loss: 0.001\n",
            "[13,   201] Train Loss: 0.106\n",
            "[13] Test Loss: 0.136\n",
            "[14,     1] Train Loss: 0.000\n",
            "[14,   201] Train Loss: 0.087\n",
            "[14] Test Loss: 0.152\n",
            "[15,     1] Train Loss: 0.000\n",
            "[15,   201] Train Loss: 0.084\n",
            "[15] Test Loss: 0.202\n",
            "[16,     1] Train Loss: 0.000\n",
            "[16,   201] Train Loss: 0.074\n",
            "[16] Test Loss: 0.161\n",
            "[17,     1] Train Loss: 0.000\n",
            "[17,   201] Train Loss: 0.065\n",
            "[17] Test Loss: 0.161\n",
            "[18,     1] Train Loss: 0.000\n",
            "[18,   201] Train Loss: 0.060\n",
            "[18] Test Loss: 0.190\n",
            "[19,     1] Train Loss: 0.001\n",
            "[19,   201] Train Loss: 0.053\n",
            "[19] Test Loss: 0.283\n",
            "[20,     1] Train Loss: 0.000\n",
            "[20,   201] Train Loss: 0.043\n",
            "[20] Test Loss: 0.220\n",
            "[21,     1] Train Loss: 0.000\n",
            "[21,   201] Train Loss: 0.038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVlCOtV6laVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(sgd_loss_train, label=\"sgd loss train\")\n",
        "plt.plot(sgd_loss_test, label=\"sgd loss test\")\n",
        "plt.plot(sgd_momentum_loss_train, label=\"sgd momentum loss train\")\n",
        "plt.plot(sgd_momentum_loss_test, label=\"sgd momentum loss test\")\n",
        "plt.plot(adam_loss_train, label=\"adam loss train\")\n",
        "plt.plot(adam_loss_test, label=\"adam loss test\")\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuU0IdPDazFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(adam_optimizer.min_step_list,label=\"min values\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "#%%\n",
        "plt.plot(adam_optimizer.max_step_list,label=\"max values\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}